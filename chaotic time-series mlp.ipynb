{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwrQUMfhsdZP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from scipy.integrate import solve_ivp\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from sklearn.preprocessing import MinMaxScaler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkmWPqqqLiqN"
      },
      "source": [
        "We used this section of the code to generate datasets of x, y, and z values of a Lorenz attractor, which is a chaotic system of equations which returns highly variable solutions with only small changes in initial values. By generating a large number of separate datasets, we hope to minimize the possibility for the model to overfit even before we started transfer learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "an4hw9JQ3n3b",
        "outputId": "a5fa83eb-908e-4bf8-9302-2c4f714127a6"
      },
      "outputs": [],
      "source": [
        "def lorenz(t, state, sigma=10, beta=8/3, rho=28):\n",
        "    x, y, z = state\n",
        "    dxdt = sigma * (y - x)\n",
        "    dydt = x * (rho - z) - y\n",
        "    dzdt = x * y - beta * z\n",
        "    return [dxdt, dydt, dzdt]\n",
        "\n",
        "# Initial conditions\n",
        "state0 = [1.0, 1.0, 1.0]\n",
        "time_span = (0, 10000)\n",
        "\n",
        "\n",
        "# Generate integer time points\n",
        "#time_eval = np.linspace(time_span[0], time_span[1], 10000) #Original Line\n",
        "time_eval = np.arange(time_span[0], time_span[1] + 1, dtype=int)  # New Line\n",
        "\n",
        "output_dir = 'lorenz_output'\n",
        "if not os.path.exists(output_dir):\n",
        "       os.makedirs(output_dir)\n",
        "\n",
        "\n",
        "for i in range(-1400,2000):\n",
        "\n",
        "    #Scrambles x y and z starting conditions to generate a new system\n",
        "    state0[i % 3] = i / 100000\n",
        "\n",
        "    # Solve the system\n",
        "    sol = solve_ivp(lorenz, time_span, state0, t_eval=time_eval)\n",
        "\n",
        "    #Save the time series data of the x y and z axes as individual csvs\n",
        "    dfx = pd.DataFrame({'Time': sol.t, 'X': sol.y[0]})\n",
        "    dfx.to_csv(\"lorenz_output_x\" + str(i) + \".csv\", index=False)\n",
        "    print(\"Data saved to lorenz_output_x\" + str(i) + \".csv\")\n",
        "\n",
        "    dfy = pd.DataFrame({'Time': sol.t, 'Y': sol.y[1]})\n",
        "    dfy.to_csv(\"lorenz_output_y \" + str(i) + \".csv\", index=False)\n",
        "    print(\"Data saved to lorenz_output_y\" + str(i) + \".csv\")\n",
        "\n",
        "    dfz = pd.DataFrame({'Time': sol.t, 'Z': sol.y[2]})\n",
        "    dfz.to_csv(\"lorenz_output_z\" + str(i) + \".csv\", index=False)\n",
        "    print(\"Data saved to lorenz_output_z\" + str(i) + \".csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2coPuj4F9gs"
      },
      "source": [
        "This section of the code combines the various Lorenz datasets for x, y and z coordinates for ease of training. The cell may not work on a new runtime, but it's not necessary for the model and algorithm to actually function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "te0UHRSwRHBX",
        "outputId": "774e374f-6193-42cf-9fa2-4a6359cf2a34"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "def combine_csv_files(directory, output_prefix):\n",
        "    \"\"\"Combines CSV files in a directory based on their prefixes (x, y, z).\"\"\"\n",
        "\n",
        "    for prefix in ['x', 'y', 'z']:\n",
        "        all_files = glob.glob(os.path.join(directory, f'lorenz_output_{prefix}*.csv'))\n",
        "        if not all_files:\n",
        "            print(f\"No files found with prefix '{prefix}' in the specified directory.\")\n",
        "            continue\n",
        "\n",
        "        combined_df = pd.concat([pd.read_csv(f) for f in all_files], ignore_index=True)\n",
        "        output_filename = os.path.join(directory, f'{output_prefix}_{prefix}.csv')\n",
        "        combined_df.to_csv(output_filename, index=False)\n",
        "        print(f\"Combined '{prefix}' files into '{output_filename}'\")\n",
        "\n",
        "# Example usage:\n",
        "combine_csv_files('/content', 'combined_lorenz')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo0AqHzBGqw7"
      },
      "source": [
        "This section of the code defines the infrastructure we need to train the model. It should be largely self-explanatory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rlZwqea4RXy"
      },
      "outputs": [],
      "source": [
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, data, sequence_length):\n",
        "        self.data = data\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.sequence_length  # Changed here\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index:index + self.sequence_length]  # Input: Past sequence_length values\n",
        "        y = self.data[index + self.sequence_length]  # Target: The next value after the sequence\n",
        "        return x, y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_hidden_layers=1):  # Added num_hidden_layers\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        layers = [nn.Linear(input_size, hidden_size), nn.ReLU()]  # Start with input layer\n",
        "\n",
        "        # Add hidden layers dynamically\n",
        "        for _ in range(num_hidden_layers):\n",
        "            layers.extend([nn.Linear(hidden_size, hidden_size), nn.ReLU()])\n",
        "\n",
        "        layers.append(nn.Linear(hidden_size, output_size))  # Add output layer\n",
        "\n",
        "        self.network = nn.Sequential(*layers)  # Unpack layers into nn.Sequential\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "# Training function (remains the same)\n",
        "def train_model(model, dataloader, criterion, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for data, targets in dataloader:\n",
        "            # Forward pass\n",
        "            outputs = model(data).squeeze(1)  # Squeeze the output tensor along dimension 1\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "def normalize_tensor(tensor, ref_tensor, feature_range=(0, 1)):\n",
        "    min_val, max_val = feature_range\n",
        "    tensor_min = ref_tensor.min(dim=0, keepdim=True)[0]\n",
        "    tensor_max = ref_tensor.max(dim=0, keepdim=True)[0]\n",
        "\n",
        "    # Normalize the tensor\n",
        "    normalized_tensor = (tensor - tensor_min) / (tensor_max - tensor_min)\n",
        "    normalized_tensor = normalized_tensor * (max_val - min_val) + min_val\n",
        "\n",
        "    return normalized_tensor\n",
        "\n",
        "def unnormalize_tensor(normalized_tensor, original_tensor, feature_range=(0, 1)):\n",
        "\n",
        "    min_val, max_val = feature_range\n",
        "    tensor_min = original_tensor.min(dim=0, keepdim=True)[0]\n",
        "    tensor_max = original_tensor.max(dim=0, keepdim=True)[0]\n",
        "\n",
        "    # Unnormalize the tensor\n",
        "    unnormalized_tensor = (normalized_tensor - min_val) / (max_val - min_val)\n",
        "    unnormalized_tensor = unnormalized_tensor * (tensor_max - tensor_min) + tensor_min\n",
        "\n",
        "    return unnormalized_tensor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOXU21pOJeV0"
      },
      "source": [
        "This section defines our hyperparameters. We adjusted these after a little experimentation from our initial values but not significantly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ofetT6v6PNb"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "hidden_size = 15\n",
        "num_hidden_layers = 3\n",
        "output_size = 1\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "learning_rate = 0.01\n",
        "sequence_length = 20  # Length of the input sequence for time-series prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMkNSJwoKAnG"
      },
      "source": [
        "This section defines the model and sets our loss and optimization functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2JXzpo7WJti"
      },
      "outputs": [],
      "source": [
        "model = MLP(sequence_length, hidden_size, output_size, num_hidden_layers)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuO3cNocLO2U"
      },
      "source": [
        "# Loading Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z58ucR9tQ0FH"
      },
      "source": [
        "This cell loads all our different data sets, normalizes them, and trains the model using those datasets. On top of the Lorenz data sets we generated, we also trained the model on various weather-based measurements for Delhi. Ideally we would have incorporated more diverse sources of real-world data but we did not have enough time to find and process usable datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NLeJ-4eALKom",
        "outputId": "f77e1e1f-8cab-4b99-9f8d-6cede1beedae"
      },
      "outputs": [],
      "source": [
        "dfx = pd.read_csv('/content/combined_lorenz_x.csv')  # Load data for 'x'\n",
        "data = torch.tensor(dfx['X'].values, dtype=torch.float32)\n",
        "data = normalize_tensor(data, data)\n",
        "dataset = TimeSeriesDataset(data,sequence_length)  # Only pass data here\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "train_model(model, dataloader, criterion, optimizer, num_epochs)\n",
        "\n",
        "dfx = pd.read_csv('/content/combined_lorenz_y.csv')  # Load data for 'x'\n",
        "data = torch.tensor(dfx['Y'].values, dtype=torch.float32)\n",
        "data = normalize_tensor(data, data)\n",
        "dataset = TimeSeriesDataset(data,sequence_length)  # Only pass data here\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "train_model(model, dataloader, criterion, optimizer, num_epochs)\n",
        "\n",
        "dfx = pd.read_csv('/content/combined_lorenz_z.csv')  # Load data for 'x'\n",
        "data = torch.tensor(dfx['Z'].values, dtype=torch.float32)\n",
        "data = normalize_tensor(data, data)\n",
        "dataset = TimeSeriesDataset(data,sequence_length)  # Only pass data here\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "train_model(model, dataloader, criterion, optimizer, num_epochs)\n",
        "\n",
        "dfx = pd.read_csv('/content/DailyDelhiClimateTrain.csv')  # Load data for 'x'\n",
        "data = torch.tensor(dfx['meantemp'].values, dtype=torch.float32)\n",
        "data = normalize_tensor(data, data)\n",
        "dataset = TimeSeriesDataset(data,sequence_length)  # Only pass data here\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "train_model(model, dataloader, criterion, optimizer, num_epochs)\n",
        "\n",
        "dfx = pd.read_csv('/content/DailyDelhiClimateTrain.csv')  # Load data for 'x'\n",
        "data = torch.tensor(dfx['humidity'].values, dtype=torch.float32)\n",
        "data = normalize_tensor(data, data)\n",
        "dataset = TimeSeriesDataset(data,sequence_length)  # Only pass data here\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "train_model(model, dataloader, criterion, optimizer, num_epochs)\n",
        "\n",
        "dfx = pd.read_csv('/content/DailyDelhiClimateTrain.csv')  # Load data for 'x'\n",
        "data = torch.tensor(dfx['wind_speed'].values, dtype=torch.float32)\n",
        "data = normalize_tensor(data, data)\n",
        "dataset = TimeSeriesDataset(data,sequence_length)  # Only pass data here\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "train_model(model, dataloader, criterion, optimizer, num_epochs)\n",
        "\n",
        "dfx = pd.read_csv('/content/DailyDelhiClimateTrain.csv')  # Load data for 'x'\n",
        "data = torch.tensor(dfx['meanpressure'].values, dtype=torch.float32)\n",
        "data = normalize_tensor(data, data)\n",
        "dataset = TimeSeriesDataset(data,sequence_length)  # Only pass data here\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "train_model(model, dataloader, criterion, optimizer, num_epochs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SOUHSj6z9Qy"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTuSy1I60B7M"
      },
      "source": [
        "# Tuning/Backtesting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8Nre8HrRNQx"
      },
      "source": [
        "Here we process the data for the transfer learning aspect of our model. For simplicity, we decided to train the model to trade only SPY, since it is liquid and traded often. If given more time we could have trained the model on different equities to allow it to be applied more generally to other equities but that was not realistic under the time constraints. Since the values of SPY have gradually grown over time, training on the normalized train data would not work very well since the model would not be able to predict a value outside of the minimum and maximum of the training data prices. Therefore, we decided to have the model predict the percent change in SPY open price day-to-day and trade on that prediction instead. There is the caveat that the opening price is not necessarily the price the model would realistically trade at, but the general idea behind the strategy shouldn't be majorly affected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtkgduq07l62",
        "outputId": "85ce209c-9f57-4261-c0d1-d393296f3427"
      },
      "outputs": [],
      "source": [
        "# Load your specific time-series financial data\n",
        "dfx = pd.read_csv('/content/HistoricalData_1739678509429.csv')\n",
        "\n",
        "print(dfx.head())\n",
        "\n",
        "# CSV data is in reverse chronological order which this code fixes\n",
        "new_df = pd.DataFrame(index=dfx.index, columns=['Open'])\n",
        "reversed_open_values = dfx['Open'].iloc[::-1].values\n",
        "new_df['Open'] = reversed_open_values\n",
        "dfx['Openpctdiff'] = new_df['Open'].pct_change()\n",
        "\n",
        "\n",
        "dfx.drop([0], inplace = True) # first value is nan\n",
        "\n",
        "# creates the train-test split\n",
        "data_train = torch.tensor(dfx['Openpctdiff'][:2004].values, dtype=torch.float32)\n",
        "data_train = normalize_tensor(data_train, data_train)\n",
        "# although there is potential for some weird outputs if the percent change exceeds the normalized range, it should be exceptionally rare\n",
        "\n",
        "data_test = torch.tensor(dfx['Openpctdiff'][2004:].values, dtype=torch.float32)\n",
        "\n",
        "actual_prices = new_df['Open']\n",
        "\n",
        "dataset = TimeSeriesDataset(data_train,sequence_length)  # Only pass data here\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkdXFg4bCoqo"
      },
      "source": [
        "This cell freezes the model weights for two of the three hidden layers, then retrain the remaining layers based on the SPY data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GN-v_GndEjf_"
      },
      "outputs": [],
      "source": [
        "for param in model.network[1].parameters():  # Freezing the first Linear layer (hidden layer)\n",
        "    param.requires_grad = False\n",
        "for param in model.network[2].parameters():  # Freezing the second Linear layer (hidden layer)\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFgPI8D7P0m4",
        "outputId": "f644e15c-871e-41a0-9c2d-2fa9594584ca"
      },
      "outputs": [],
      "source": [
        "train_model(model, dataloader, criterion, optimizer, num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-UTH2oyIjxv"
      },
      "source": [
        "In this section we implemented a rolling window backtesting method and calculated the PnL of our model on the test SPY price data. We used a fairly simplistic trading signal: if the model predicted that the next period's open price would be a percent change in price outside of a certain range, our model would trade accordingly. For simplicity's sake our model only predicted day by day and would close the position at the next day's open. To calculate our PnL, we subtracted the current price from the price of the next period, adding or subtracting that from PnL depending on what direction the model recommended we trade. The values defining this range were chosen from backtesting. Given more time we may have been able to come up with a more interesting and complex trading signal, but the one we use should serve our purposes fine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "3lNWY1sK0Igd",
        "outputId": "764b4f11-ee55-42a1-a456-e153b1f9efad"
      },
      "outputs": [],
      "source": [
        "def backtest_model(model, data_train, data_test, actual_prices, window_size=50, percent_diff=1.0):\n",
        "    pnl = 0\n",
        "    results = []\n",
        "    # Normalize the test data\n",
        "    data_test_normalized = normalize_tensor(data_test, ref_tensor=data_train) # need to keep same normalization standard since the model wouldn't have access to future data and therefore shouldn't be normalized on the entire test dataset\n",
        "\n",
        "    for i in range(window_size, len(data_test_normalized)):\n",
        "        X_test = data_test_normalized[i-window_size:i].unsqueeze(0)\n",
        "\n",
        "        for j in range(i - sequence_length):\n",
        "          X_input = X_test[0][j:j+sequence_length]\n",
        "\n",
        "        # Test the model and get prediction\n",
        "          model.eval()\n",
        "          with torch.no_grad():\n",
        "            prediction = model(X_input)\n",
        "\n",
        "          if tradingsignal(prediction) == 1:\n",
        "              # Get actual price at the predicted date\n",
        "            actual_future_price = actual_prices[i + 1]\n",
        "\n",
        "          elif tradingsignal(prediction) == -1:\n",
        "            actual_future_price = actual_prices[i + 1]\n",
        "            pnl -= 100 * (actual_future_price - actual_prices[i])\n",
        "\n",
        "            if pnl>0:\n",
        "              wins+=1\n",
        "            else:\n",
        "              losses+=1\n",
        "\n",
        "\n",
        "          if X_input[-1] == X_test[0][-1]:\n",
        "            break\n",
        "\n",
        "    winrate = wins/(wins+losses)\n",
        "    print(f'Winrate: {winrate}')\n",
        "\n",
        "    return pnl\n",
        "\n",
        "\n",
        "def tradingsignal(predicted):\n",
        "    if unnormalize_tensor(predicted, data_test)[0] > 0.012:\n",
        "        return 1\n",
        "    elif unnormalize_tensor(predicted, data_test)[0] < -0.012:\n",
        "        return -1\n",
        "    return 0\n",
        "\n",
        "\n",
        "pnl = backtest_model(model, data_train, data_test, actual_prices, window_size = 50)\n",
        "print(f'PnL: {pnl}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ihUKXpKewD-"
      },
      "source": [
        "The second to last cell saves our model, which is attached to the email. To actually load the model, run the last cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3Fo9cplA85c"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'mlp_model.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yfvlMOWBXOb"
      },
      "outputs": [],
      "source": [
        "model = torch.load('mlp_model.pth')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The general thought process behind our idea is not different from the writeup we submitted which is pasted below. Some details of the actual model/strategy are different than what the writeup says, but the overall idea is intact.\n",
        "\n",
        "Idea:\n",
        "Our idea is to train a multi-layer perceptron (MLP) to predict chaotic systems and retrain it specifically on financial market data of some liquid asset. By first training the model on general chaotic systems, we hope that the model will be able to better generalize its behavior and predictions compared to training it purely on the historical financial data of some security. The chaotic systems the model is trained on can either be self-generated (Lorenz systems, Henón maps, Mackey-Glass equations, etc.) or natural (weather patterns, animal populations, biological systems, etc.). Our choice of using MLPs comes from chaotic systems research by our group member Badis Labbedi. When the predicted future price of the instrument exceeds a set deviation from the actual current value, we will trade against the direction of deviation and exit the position when the price falls back within our bounds. We will track the delta between the initial price and the final price among consistent time intervals (which will stay constant between training and testing).\n",
        "\n",
        "\n",
        "Synthetic chaos:\n",
        "We can generate our own datasets using pre-established equations that describe chaotic systems, which we can then feed into the model. Additionally, the wide range of possibilities means that there will be more unique data that the model can be trained with, allowing it to increase its accuracy while avoiding overfitting on any particular dataset. We also have a theoretically infinite amount of this data since we can just adjust the starting conditions and other parameters of the equations to generate an entirely new chaotic system.\n",
        "\n",
        "Natural chaos:\n",
        "We can also find natural instances of chaotic systems and train our model on that as well. These have the added benefit of being more “truly” chaotic compared to data generated by equations, so the results of training on these datasets may be more realistic compared to the synthetic data. The caveats here are that the data may be very messy or difficult to use in its raw form, meaning that we will have to go through a lot of trouble to process it. Additionally, there is not a ton of publicly available data fitting our criteria in general, so the benefits of our approach are rather limited.\n",
        "\n",
        "Adaptation to markets:\n",
        "After training on natural chaotic datasets and synthetic chaos datasets, we’ll remove the output layer and then create a new output layer which will be trained upon the financial data of some index or other security. We have yet to determine specifically what we want our model to trade, but for it to fit our criteria it should probably be a very liquid asset like an S&P 500 Index tracker or some large-cap stock.\n",
        "\n",
        "Backtesting/Tuning:\n",
        "In backtesting, we will primarily seek to tune the time period of signal generation where the model performs optimally and the execution threshold for the proportional difference between the modeled price and real price for risk-adjusted returns.\n",
        "\n",
        "\n",
        "Deployment:\n",
        "We plan on feeding real-time financial data through our model by using API’s like databento or whatever we find to work best. We expect real-world execution to deviate from our predicted p/l as broker execution will be delayed due to hardware and brokerage constraints.\n",
        "\n",
        "Resources/Proof of Concept\n",
        "\n",
        "https://www.mdpi.com/2227-7390/12/12/1920\n",
        "https://link.springer.com/article/10.1007/s43069-021-00071-2\n",
        "https://www.researchgate.net/publication/380583239_MLP_and_RBF_Algorithms_in_Finance_Predicting_and_Classifying_Stock_Prices_amidst_Economic_Policy_Uncertainty\n",
        "\n"
      ],
      "metadata": {
        "id": "AUSm8vpLctuW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwrQUMfhsdZP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from scipy.integrate import solve_ivp\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from sklearn.preprocessing import MinMaxScaler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkmWPqqqLiqN"
      },
      "source": [
        "We used this section of the code to generate datasets of x, y, and z values of a Lorenz attractor, which is a chaotic system of equations which returns highly variable solutions with only small changes in initial values. By generating a large number of separate datasets, we hope to minimize the possibility for the model to overfit even before we started transfer learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "an4hw9JQ3n3b",
        "outputId": "a5fa83eb-908e-4bf8-9302-2c4f714127a6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c86b74d0f705>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Solve the system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0msol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolve_ivp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlorenz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_span\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m#Save the time series data of the x y and z axes as individual csvs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/integrate/_ivp/ivp.py\u001b[0m in \u001b[0;36msolve_ivp\u001b[0;34m(fun, t_span, y0, method, t_eval, dense_output, events, vectorized, args, **options)\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'finished'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/integrate/_ivp/base.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/integrate/_ivp/rk.py\u001b[0m in \u001b[0;36m_step_impl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mh_abs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             y_new, f_new = rk_step(self.fun, t, y, self.f, h, self.A,\n\u001b[0m\u001b[1;32m    145\u001b[0m                                    self.B, self.C, self.K)\n\u001b[1;32m    146\u001b[0m             \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matol\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/integrate/_ivp/rk.py\u001b[0m in \u001b[0;36mrk_step\u001b[0;34m(fun, t, y, f, h, A, B, C, K)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mdy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0my_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def lorenz(t, state, sigma=10, beta=8/3, rho=28):\n",
        "    x, y, z = state\n",
        "    dxdt = sigma * (y - x)\n",
        "    dydt = x * (rho - z) - y\n",
        "    dzdt = x * y - beta * z\n",
        "    return [dxdt, dydt, dzdt]\n",
        "\n",
        "# Initial conditions\n",
        "state0 = [1.0, 1.0, 1.0]\n",
        "time_span = (0, 10000)\n",
        "\n",
        "\n",
        "# Generate integer time points\n",
        "#time_eval = np.linspace(time_span[0], time_span[1], 10000) #Original Line\n",
        "time_eval = np.arange(time_span[0], time_span[1] + 1, dtype=int)  # New Line\n",
        "\n",
        "output_dir = 'lorenz_output'\n",
        "if not os.path.exists(output_dir):\n",
        "       os.makedirs(output_dir)\n",
        "\n",
        "\n",
        "for i in range(-1400,2000):\n",
        "\n",
        "    #Scrambles x y and z starting conditions to generate a new system\n",
        "    state0[i % 3] = i / 100000\n",
        "\n",
        "    # Solve the system\n",
        "    sol = solve_ivp(lorenz, time_span, state0, t_eval=time_eval)\n",
        "\n",
        "    #Save the time series data of the x y and z axes as individual csvs\n",
        "    dfx = pd.DataFrame({'Time': sol.t, 'X': sol.y[0]})\n",
        "    dfx.to_csv(\"lorenz_output_x\" + str(i) + \".csv\", index=False)\n",
        "    print(\"Data saved to lorenz_output_x\" + str(i) + \".csv\")\n",
        "\n",
        "    dfy = pd.DataFrame({'Time': sol.t, 'Y': sol.y[1]})\n",
        "    dfy.to_csv(\"lorenz_output_y \" + str(i) + \".csv\", index=False)\n",
        "    print(\"Data saved to lorenz_output_y\" + str(i) + \".csv\")\n",
        "\n",
        "    dfz = pd.DataFrame({'Time': sol.t, 'Z': sol.y[2]})\n",
        "    dfz.to_csv(\"lorenz_output_z\" + str(i) + \".csv\", index=False)\n",
        "    print(\"Data saved to lorenz_output_z\" + str(i) + \".csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section of the code combines the various Lorenz datasets for x, y and z coordinates for ease of training. The cell may not work on a new runtime, but it's not necessary for the model and algorithm to actually function."
      ],
      "metadata": {
        "id": "o2coPuj4F9gs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "te0UHRSwRHBX",
        "outputId": "774e374f-6193-42cf-9fa2-4a6359cf2a34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No files found with prefix 'x' in the specified directory.\n",
            "No files found with prefix 'y' in the specified directory.\n",
            "No files found with prefix 'z' in the specified directory.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "def combine_csv_files(directory, output_prefix):\n",
        "    \"\"\"Combines CSV files in a directory based on their prefixes (x, y, z).\"\"\"\n",
        "\n",
        "    for prefix in ['x', 'y', 'z']:\n",
        "        all_files = glob.glob(os.path.join(directory, f'lorenz_output_{prefix}*.csv'))\n",
        "        if not all_files:\n",
        "            print(f\"No files found with prefix '{prefix}' in the specified directory.\")\n",
        "            continue\n",
        "\n",
        "        combined_df = pd.concat([pd.read_csv(f) for f in all_files], ignore_index=True)\n",
        "        output_filename = os.path.join(directory, f'{output_prefix}_{prefix}.csv')\n",
        "        combined_df.to_csv(output_filename, index=False)\n",
        "        print(f\"Combined '{prefix}' files into '{output_filename}'\")\n",
        "\n",
        "# Example usage:\n",
        "combine_csv_files('/content', 'combined_lorenz')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section of the code defines the infrastructure we need to train the model. It should be largely self-explanatory."
      ],
      "metadata": {
        "id": "qo0AqHzBGqw7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rlZwqea4RXy"
      },
      "outputs": [],
      "source": [
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, data, sequence_length):\n",
        "        self.data = data\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.sequence_length  # Changed here\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index:index + self.sequence_length]  # Input: Past sequence_length values\n",
        "        y = self.data[index + self.sequence_length]  # Target: The next value after the sequence\n",
        "        return x, y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_hidden_layers=1):  # Added num_hidden_layers\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        layers = [nn.Linear(input_size, hidden_size), nn.ReLU()]  # Start with input layer\n",
        "\n",
        "        # Add hidden layers dynamically\n",
        "        for _ in range(num_hidden_layers):\n",
        "            layers.extend([nn.Linear(hidden_size, hidden_size), nn.ReLU()])\n",
        "\n",
        "        layers.append(nn.Linear(hidden_size, output_size))  # Add output layer\n",
        "\n",
        "        self.network = nn.Sequential(*layers)  # Unpack layers into nn.Sequential\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "# Training function (remains the same)\n",
        "def train_model(model, dataloader, criterion, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for data, targets in dataloader:\n",
        "            # Forward pass\n",
        "            outputs = model(data).squeeze(1)  # Squeeze the output tensor along dimension 1\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "def normalize_tensor(tensor, ref_tensor, feature_range=(0, 1)):\n",
        "    min_val, max_val = feature_range\n",
        "    tensor_min = ref_tensor.min(dim=0, keepdim=True)[0]\n",
        "    tensor_max = ref_tensor.max(dim=0, keepdim=True)[0]\n",
        "\n",
        "    # Normalize the tensor\n",
        "    normalized_tensor = (tensor - tensor_min) / (tensor_max - tensor_min)\n",
        "    normalized_tensor = normalized_tensor * (max_val - min_val) + min_val\n",
        "\n",
        "    return normalized_tensor\n",
        "\n",
        "def unnormalize_tensor(normalized_tensor, original_tensor, feature_range=(0, 1)):\n",
        "\n",
        "    min_val, max_val = feature_range\n",
        "    tensor_min = original_tensor.min(dim=0, keepdim=True)[0]\n",
        "    tensor_max = original_tensor.max(dim=0, keepdim=True)[0]\n",
        "\n",
        "    # Unnormalize the tensor\n",
        "    unnormalized_tensor = (normalized_tensor - min_val) / (max_val - min_val)\n",
        "    unnormalized_tensor = unnormalized_tensor * (tensor_max - tensor_min) + tensor_min\n",
        "\n",
        "    return unnormalized_tensor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section defines our hyperparameters. We adjusted these after a little experimentation from our initial values but not significantly."
      ],
      "metadata": {
        "id": "QOXU21pOJeV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "hidden_size = 15\n",
        "num_hidden_layers = 3\n",
        "output_size = 1\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "learning_rate = 0.01\n",
        "sequence_length = 20  # Length of the input sequence for time-series prediction"
      ],
      "metadata": {
        "id": "_ofetT6v6PNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section defines the model and sets our loss and optimization functions."
      ],
      "metadata": {
        "id": "QMkNSJwoKAnG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2JXzpo7WJti"
      },
      "outputs": [],
      "source": [
        "model = MLP(sequence_length, hidden_size, output_size, num_hidden_layers)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuO3cNocLO2U"
      },
      "source": [
        "# Loading Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell loads all our different data sets, normalizes them, and trains the model using those datasets. On top of the Lorenz data sets we generated, we also trained the model on various weather-based measurements for Delhi. Ideally we would have incorporated more diverse sources of real-world data but we did not have enough time to find and process usable datasets."
      ],
      "metadata": {
        "id": "Z58ucR9tQ0FH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLeJ-4eALKom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f77e1e1f-8cab-4b99-9f8d-6cede1beedae",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.0434\n",
            "Epoch [2/10], Loss: 0.0415\n",
            "Epoch [3/10], Loss: 0.0413\n",
            "Epoch [4/10], Loss: 0.0413\n",
            "Epoch [5/10], Loss: 0.0413\n",
            "Epoch [6/10], Loss: 0.0414\n",
            "Epoch [7/10], Loss: 0.0420\n",
            "Epoch [8/10], Loss: 0.0408\n",
            "Epoch [9/10], Loss: 0.0410\n",
            "Epoch [10/10], Loss: 0.0373\n",
            "Epoch [1/10], Loss: 0.0207\n",
            "Epoch [2/10], Loss: 0.0206\n",
            "Epoch [3/10], Loss: 0.0206\n",
            "Epoch [4/10], Loss: 0.0200\n",
            "Epoch [5/10], Loss: 0.0197\n",
            "Epoch [6/10], Loss: 0.0194\n",
            "Epoch [7/10], Loss: 0.0193\n",
            "Epoch [8/10], Loss: 0.0193\n",
            "Epoch [9/10], Loss: 0.0195\n",
            "Epoch [10/10], Loss: 0.0192\n",
            "Epoch [1/10], Loss: 0.0198\n",
            "Epoch [2/10], Loss: 0.0196\n",
            "Epoch [3/10], Loss: 0.0197\n",
            "Epoch [4/10], Loss: 0.0208\n",
            "Epoch [5/10], Loss: 0.0210\n",
            "Epoch [6/10], Loss: 0.0199\n",
            "Epoch [7/10], Loss: 0.0209\n",
            "Epoch [8/10], Loss: 0.0206\n",
            "Epoch [9/10], Loss: 0.0207\n",
            "Epoch [10/10], Loss: 0.0191\n",
            "Epoch [1/10], Loss: 0.1022\n",
            "Epoch [2/10], Loss: 0.0557\n",
            "Epoch [3/10], Loss: 0.0672\n",
            "Epoch [4/10], Loss: 0.0638\n",
            "Epoch [5/10], Loss: 0.0644\n",
            "Epoch [6/10], Loss: 0.0642\n",
            "Epoch [7/10], Loss: 0.0641\n",
            "Epoch [8/10], Loss: 0.0641\n",
            "Epoch [9/10], Loss: 0.0641\n",
            "Epoch [10/10], Loss: 0.0640\n",
            "Epoch [1/10], Loss: 0.0324\n",
            "Epoch [2/10], Loss: 0.0344\n",
            "Epoch [3/10], Loss: 0.0329\n",
            "Epoch [4/10], Loss: 0.0331\n",
            "Epoch [5/10], Loss: 0.0332\n",
            "Epoch [6/10], Loss: 0.0332\n",
            "Epoch [7/10], Loss: 0.0331\n",
            "Epoch [8/10], Loss: 0.0331\n",
            "Epoch [9/10], Loss: 0.0331\n",
            "Epoch [10/10], Loss: 0.0331\n",
            "Epoch [1/10], Loss: 0.0131\n",
            "Epoch [2/10], Loss: 0.0093\n",
            "Epoch [3/10], Loss: 0.0098\n",
            "Epoch [4/10], Loss: 0.0097\n",
            "Epoch [5/10], Loss: 0.0095\n",
            "Epoch [6/10], Loss: 0.0095\n",
            "Epoch [7/10], Loss: 0.0095\n",
            "Epoch [8/10], Loss: 0.0095\n",
            "Epoch [9/10], Loss: 0.0095\n",
            "Epoch [10/10], Loss: 0.0095\n",
            "Epoch [1/10], Loss: 0.0000\n",
            "Epoch [2/10], Loss: 0.0000\n",
            "Epoch [3/10], Loss: 0.0000\n",
            "Epoch [4/10], Loss: 0.0000\n",
            "Epoch [5/10], Loss: 0.0000\n",
            "Epoch [6/10], Loss: 0.0000\n",
            "Epoch [7/10], Loss: 0.0000\n",
            "Epoch [8/10], Loss: 0.0000\n",
            "Epoch [9/10], Loss: 0.0000\n",
            "Epoch [10/10], Loss: 0.0000\n"
          ]
        }
      ],
      "source": [
        "dfx = pd.read_csv('/content/combined_lorenz_x.csv')  # Load data for 'x'\n",
        "data = torch.tensor(dfx['X'].values, dtype=torch.float32)\n",
        "data = normalize_tensor(data, data)\n",
        "dataset = TimeSeriesDataset(data,sequence_length)  # Only pass data here\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "train_model(model, dataloader, criterion, optimizer, num_epochs)\n",
        "\n",
        "dfx = pd.read_csv('/content/combined_lorenz_y.csv')  # Load data for 'x'\n",
        "data = torch.tensor(dfx['Y'].values, dtype=torch.float32)\n",
        "data = normalize_tensor(data, data)\n",
        "dataset = TimeSeriesDataset(data,sequence_length)  # Only pass data here\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "train_model(model, dataloader, criterion, optimizer, num_epochs)\n",
        "\n",
        "dfx = pd.read_csv('/content/combined_lorenz_z.csv')  # Load data for 'x'\n",
        "data = torch.tensor(dfx['Z'].values, dtype=torch.float32)\n",
        "data = normalize_tensor(data, data)\n",
        "dataset = TimeSeriesDataset(data,sequence_length)  # Only pass data here\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "train_model(model, dataloader, criterion, optimizer, num_epochs)\n",
        "\n",
        "dfx = pd.read_csv('/content/DailyDelhiClimateTrain.csv')  # Load data for 'x'\n",
        "data = torch.tensor(dfx['meantemp'].values, dtype=torch.float32)\n",
        "data = normalize_tensor(data, data)\n",
        "dataset = TimeSeriesDataset(data,sequence_length)  # Only pass data here\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "train_model(model, dataloader, criterion, optimizer, num_epochs)\n",
        "\n",
        "dfx = pd.read_csv('/content/DailyDelhiClimateTrain.csv')  # Load data for 'x'\n",
        "data = torch.tensor(dfx['humidity'].values, dtype=torch.float32)\n",
        "data = normalize_tensor(data, data)\n",
        "dataset = TimeSeriesDataset(data,sequence_length)  # Only pass data here\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "train_model(model, dataloader, criterion, optimizer, num_epochs)\n",
        "\n",
        "dfx = pd.read_csv('/content/DailyDelhiClimateTrain.csv')  # Load data for 'x'\n",
        "data = torch.tensor(dfx['wind_speed'].values, dtype=torch.float32)\n",
        "data = normalize_tensor(data, data)\n",
        "dataset = TimeSeriesDataset(data,sequence_length)  # Only pass data here\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "train_model(model, dataloader, criterion, optimizer, num_epochs)\n",
        "\n",
        "dfx = pd.read_csv('/content/DailyDelhiClimateTrain.csv')  # Load data for 'x'\n",
        "data = torch.tensor(dfx['meanpressure'].values, dtype=torch.float32)\n",
        "data = normalize_tensor(data, data)\n",
        "dataset = TimeSeriesDataset(data,sequence_length)  # Only pass data here\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "train_model(model, dataloader, criterion, optimizer, num_epochs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SOUHSj6z9Qy"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTuSy1I60B7M"
      },
      "source": [
        "# Tuning/Backtesting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we process the data for the transfer learning aspect of our model. For simplicity, we decided to train the model to trade only SPY, since it is liquid and traded often. If given more time we could have trained the model on different equities to allow it to be applied more generally to other equities but that was not realistic under the time constraints. Since the values of SPY have gradually grown over time, training on the normalized train data would not work very well since the model would not be able to predict a value outside of the minimum and maximum of the training data prices. Therefore, we decided to have the model predict the percent change in SPY open price day-to-day and trade on that prediction instead. There is the caveat that the opening price is not necessarily the price the model would realistically trade at, but the general idea behind the strategy shouldn't be majorly affected."
      ],
      "metadata": {
        "id": "A8Nre8HrRNQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your specific time-series financial data\n",
        "dfx = pd.read_csv('/content/HistoricalData_1739678509429.csv')\n",
        "\n",
        "print(dfx.head())\n",
        "\n",
        "# CSV data is in reverse chronological order which this code fixes\n",
        "new_df = pd.DataFrame(index=dfx.index, columns=['Open'])\n",
        "reversed_open_values = dfx['Open'].iloc[::-1].values\n",
        "new_df['Open'] = reversed_open_values\n",
        "dfx['Openpctdiff'] = new_df['Open'].pct_change()\n",
        "\n",
        "\n",
        "dfx.drop([0], inplace = True) # first value is nan\n",
        "\n",
        "# creates the train-test split\n",
        "data_train = torch.tensor(dfx['Openpctdiff'][:2004].values, dtype=torch.float32)\n",
        "data_train = normalize_tensor(data_train, data_train)\n",
        "# although there is potential for some weird outputs if the percent change exceeds the normalized range, it should be exceptionally rare\n",
        "\n",
        "data_test = torch.tensor(dfx['Openpctdiff'][2004:].values, dtype=torch.float32)\n",
        "\n",
        "actual_prices = new_df['Open']\n",
        "\n",
        "dataset = TimeSeriesDataset(data_train,sequence_length)  # Only pass data here\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtkgduq07l62",
        "outputId": "85ce209c-9f57-4261-c0d1-d393296f3427"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Date  Close/Last    Volume    Open      High     Low\n",
            "0  02/14/2025      609.70  26910450  609.94  610.9900  609.07\n",
            "1  02/13/2025      609.73  40921300  604.48  609.9400  603.20\n",
            "2  02/12/2025      603.36  45076080  599.20  604.5500  598.51\n",
            "3  02/11/2025      605.31  30056740  602.55  605.8600  602.43\n",
            "4  02/10/2025      604.85  26048710  604.03  605.4999  602.74\n",
            "1       0.001242\n",
            "2      -0.001192\n",
            "3       0.000334\n",
            "4       0.006970\n",
            "5       0.000853\n",
            "          ...   \n",
            "2511   -0.004713\n",
            "2512   -0.002450\n",
            "2513   -0.005560\n",
            "2514    0.008812\n",
            "2515    0.009033\n",
            "Name: Openpctdiff, Length: 2515, dtype: float64\n",
            "tensor([0.5891, 0.5730, 0.5831,  ..., 0.5669, 0.5535, 0.6480])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell freezes the model weights for two of the three hidden layers, then retrain the remaining layers based on the SPY data."
      ],
      "metadata": {
        "id": "fkdXFg4bCoqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.network[1].parameters():  # Freezing the first Linear layer (hidden layer)\n",
        "    param.requires_grad = False\n",
        "for param in model.network[2].parameters():  # Freezing the second Linear layer (hidden layer)\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "GN-v_GndEjf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, dataloader, criterion, optimizer, num_epochs)"
      ],
      "metadata": {
        "id": "HFgPI8D7P0m4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f644e15c-871e-41a0-9c2d-2fa9594584ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.0089\n",
            "Epoch [2/10], Loss: 0.0079\n",
            "Epoch [3/10], Loss: 0.0080\n",
            "Epoch [4/10], Loss: 0.0079\n",
            "Epoch [5/10], Loss: 0.0079\n",
            "Epoch [6/10], Loss: 0.0079\n",
            "Epoch [7/10], Loss: 0.0079\n",
            "Epoch [8/10], Loss: 0.0079\n",
            "Epoch [9/10], Loss: 0.0079\n",
            "Epoch [10/10], Loss: 0.0079\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we implemented a rolling window backtesting method and calculated the PnL of our model on the test SPY price data. We used a fairly simplistic trading signal: if the model predicted that the next period's open price would be a percent change in price outside of a certain range, our model would trade accordingly. For simplicity's sake our model only predicted day by day and would close the position at the next day's open. To calculate our PnL, we subtracted the current price from the price of the next period, adding or subtracting that from PnL depending on what direction the model recommended we trade. The values defining this range were chosen from backtesting. Given more time we may have been able to come up with a more interesting and complex trading signal, but the one we use should serve our purposes fine."
      ],
      "metadata": {
        "id": "g-UTH2oyIjxv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lNWY1sK0Igd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "764b4f11-ee55-42a1-a456-e153b1f9efad"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-229a015919a5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mpnl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbacktest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactual_prices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'PnL: {pnl}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "def backtest_model(model, data_train, data_test, actual_prices, window_size=50, percent_diff=1.0):\n",
        "    pnl = 0\n",
        "    results = []\n",
        "    # Normalize the test data\n",
        "    data_test_normalized = normalize_tensor(data_test, ref_tensor=data_train) # need to keep same normalization standard since the model wouldn't have access to future data and therefore shouldn't be normalized on the entire test dataset\n",
        "\n",
        "    for i in range(window_size, len(data_test_normalized)):\n",
        "        X_test = data_test_normalized[i-window_size:i].unsqueeze(0)\n",
        "\n",
        "        for j in range(i - sequence_length):\n",
        "          X_input = X_test[0][j:j+sequence_length]\n",
        "\n",
        "        # Test the model and get prediction\n",
        "          model.eval()\n",
        "          with torch.no_grad():\n",
        "            prediction = model(X_input)\n",
        "\n",
        "          if tradingsignal(prediction) == 1:\n",
        "              # Get actual price at the predicted date\n",
        "            actual_future_price = actual_prices[i + 1]\n",
        "\n",
        "          elif tradingsignal(prediction) == -1:\n",
        "            actual_future_price = actual_prices[i + 1]\n",
        "            pnl -= 100 * (actual_future_price - actual_prices[i])\n",
        "\n",
        "            if pnl>0:\n",
        "              wins+=1\n",
        "            else:\n",
        "              losses+=1\n",
        "\n",
        "\n",
        "          if X_input[-1] == X_test[0][-1]:\n",
        "            break\n",
        "\n",
        "    winrate = wins/(wins+losses)\n",
        "    print(f'Winrate: {winrate}')\n",
        "\n",
        "    return pnl\n",
        "\n",
        "\n",
        "def tradingsignal(predicted):\n",
        "    if unnormalize_tensor(predicted, data_test)[0] > 0.012:\n",
        "        return 1\n",
        "    elif unnormalize_tensor(predicted, data_test)[0] < -0.012:\n",
        "        return -1\n",
        "    return 0\n",
        "\n",
        "\n",
        "pnl = backtest_model(model, data_train, data_test, actual_prices, window_size = 50)\n",
        "print(f'PnL: {pnl}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The second to last cell saves our model, which is attached to the email. To actually load the model, run the last cell."
      ],
      "metadata": {
        "id": "9ihUKXpKewD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'mlp_model.pth')\n"
      ],
      "metadata": {
        "id": "I3Fo9cplA85c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('mlp_model.pth')"
      ],
      "metadata": {
        "id": "_yfvlMOWBXOb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
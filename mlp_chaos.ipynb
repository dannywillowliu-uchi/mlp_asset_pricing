{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwrQUMfhsdZP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from scipy.integrate import solve_ivp\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lorenz func data gen\n",
        "\n",
        "outputs csvs with format time, Var where time is an integer, and Var is the value of given Var at t"
      ],
      "metadata": {
        "id": "BkmWPqqqLiqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def lorenz(t, state, sigma=10, beta=8/3, rho=28):\n",
        "    x, y, z = state\n",
        "    dxdt = sigma * (y - x)\n",
        "    dydt = x * (rho - z) - y\n",
        "    dzdt = x * y - beta * z\n",
        "    return [dxdt, dydt, dzdt]\n",
        "\n",
        "# Initial conditions\n",
        "state0 = [1.0, 1.0, 1.0]\n",
        "time_span = (0, 10000)\n",
        "\n",
        "\n",
        "# Generate integer time points\n",
        "#time_eval = np.linspace(time_span[0], time_span[1], 10000) #Original Line\n",
        "time_eval = np.arange(time_span[0], time_span[1] + 1, dtype=int)  # New Line\n",
        "\n",
        "output_dir = 'lorenz_output'\n",
        "if not os.path.exists(output_dir):\n",
        "       os.makedirs(output_dir)\n",
        "\n",
        "\n",
        "for i in range(-2000,2000):\n",
        "\n",
        "    #Scrambles x y and z starting conditions to generate a new system\n",
        "    state0[i % 3] == i / 100000\n",
        "\n",
        "    # Solve the system\n",
        "    sol = solve_ivp(lorenz, time_span, state0, t_eval=time_eval)\n",
        "\n",
        "    #Save the time series data of the x y and z axes as individual csvs\n",
        "    dfx = pd.DataFrame({'Time': sol.t, 'X': sol.y[0]})\n",
        "    dfx.to_csv(\"lorenz_output_x\" + str(i) + \".csv\", index=False)\n",
        "    print(\"Data saved to lorenz_output_x\" + str(i) + \".csv\")\n",
        "\n",
        "    dfy = pd.DataFrame({'Time': sol.t, 'Y': sol.y[1]})\n",
        "    dfy.to_csv(\"lorenz_output_y \" + str(i) + \".csv\", index=False)\n",
        "    print(\"Data saved to lorenz_output_y\" + str(i) + \".csv\")\n",
        "\n",
        "    dfz = pd.DataFrame({'Time': sol.t, 'Z': sol.y[2]})\n",
        "    dfz.to_csv(\"lorenz_output_z\" + str(i) + \".csv\", index=False)\n",
        "    print(\"Data saved to lorenz_output_z\" + str(i) + \".csv\")"
      ],
      "metadata": {
        "id": "an4hw9JQ3n3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "def combine_csv_files(directory, output_prefix):\n",
        "    \"\"\"Combines CSV files in a directory based on their prefixes (x, y, z).\"\"\"\n",
        "\n",
        "    for prefix in ['x', 'y', 'z']:\n",
        "        all_files = glob.glob(os.path.join(directory, f'lorenz_output_{prefix}*.csv'))\n",
        "        if not all_files:\n",
        "            print(f\"No files found with prefix '{prefix}' in the specified directory.\")\n",
        "            continue\n",
        "\n",
        "        combined_df = pd.concat([pd.read_csv(f) for f in all_files], ignore_index=True)\n",
        "        output_filename = os.path.join(directory, f'{output_prefix}_{prefix}.csv')\n",
        "        combined_df.to_csv(output_filename, index=False)\n",
        "        print(f\"Combined '{prefix}' files into '{output_filename}'\")\n",
        "\n",
        "# Example usage:\n",
        "combine_csv_files('/content', 'combined_lorenz')\n"
      ],
      "metadata": {
        "id": "te0UHRSwRHBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, data, sequence_length):\n",
        "        self.data = data\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.sequence_length  # Changed here\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index:index + self.sequence_length]  # Input: Past sequence_length values\n",
        "        y = self.data[index + self.sequence_length]  # Target: The next value after the sequence\n",
        "        return x, y\n",
        "\n",
        "\n",
        "# Define the MLP model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):  # Changed input parameters\n",
        "        super(MLP, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),  # Input layer with input_size features\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),  # Hidden layer with hidden_size features\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, output_size)   # Output layer with output_size features\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "\n",
        "# Training function (remains the same)\n",
        "def train_model(model, dataloader, criterion, optimizer, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        for data, targets in dataloader:\n",
        "            # Forward pass\n",
        "            outputs = model(data).squeeze(1)  # Squeeze the output tensor along dimension 1\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "id": "8rlZwqea4RXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Data"
      ],
      "metadata": {
        "id": "GuO3cNocLO2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfx = pd.read_csv(\"/content/combined_lorenz_x.csv\")  # Load data for 'x'\n",
        "data = torch.tensor(dfx['X'].values, dtype=torch.float32)\n",
        "targets = torch.randn(100, 1)  # 100 targets\n",
        "sequence_length = 5\n",
        "# Hyperparameters\n",
        "hidden_size = 50\n",
        "output_size = 1\n",
        "num_epochs = 10\n",
        "batch_size = 10\n",
        "learning_rate = 0.1\n",
        "sequence_length = 20  # Length of the input sequence for time-series prediction\n",
        "# Dataset and DataLoader\n",
        "dataset = TimeSeriesDataset(data,sequence_length)  # Only pass data here\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Model, criterion, and optimizer\n",
        "model = MLP(sequence_length, hidden_size, 1)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
      ],
      "metadata": {
        "id": "NLeJ-4eALKom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_idx, (data, targets) in enumerate(dataloader):\n",
        "    print(f\"Batch {batch_idx}:\")\n",
        "    print(\"Data:\", data)\n",
        "    print(\"Targets:\", targets)\n",
        "    break  # Stop after the first batch to avoid printing too much"
      ],
      "metadata": {
        "id": "bbClIkZCOCqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# training"
      ],
      "metadata": {
        "id": "FpSPz5VxLRuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, dataloader, criterion, optimizer, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPCTRsb4LLNb",
        "outputId": "bc67560a-968d-479e-c3a8-ab0494bdd4d3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 73.0187\n",
            "Epoch [2/10], Loss: 78.2009\n"
          ]
        }
      ]
    }
  ]
}